{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08 — Time Series (Univariate + Exogenous) — PyCaret 3\n",
        "**Dataset:** Beijing PM2.5 with weather  \n",
        "**Goal:** Forecast next 24 hours using exogenous features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.11 (main, Feb  5 2025, 19:11:07) [Clang 19.1.6 ]\n",
            "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
            "scikit-learn: 1.4.2\n",
            "pycaret: 3.3.2\n",
            "\n",
            "GPU detected:\n",
            "Sun Nov  2 20:31:48 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.65.05              Driver Version: 580.88         CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3060 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
            "|  0%   31C    P8             10W /  200W |    1002MiB /   8192MiB |      5%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install latest stable PyCaret 3.x and common GPU-capable libs\n",
        "# !pip -q install --upgrade pycaret xgboost catboost lightgbm plotly gradio\n",
        "\n",
        "import sys, sklearn, pycaret, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"pycaret:\", pycaret.__version__)\n",
        "\n",
        "# Check GPU\n",
        "try:\n",
        "    import subprocess, shutil\n",
        "    if shutil.which(\"nvidia-smi\"):\n",
        "        print(\"\\nGPU detected:\")\n",
        "        print(subprocess.check_output([\"nvidia-smi\"], text=True))\n",
        "    else:\n",
        "        print(\"\\nNo NVIDIA GPU detected in this runtime. (Colab: Runtime -> Change runtime type -> GPU)\")\n",
        "except Exception as e:\n",
        "    print(\"GPU check error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\")\n",
        "df['datetime'] = pd.to_datetime(df[['year','month','day','hour']])\n",
        "df = df[['datetime','pm2.5','DEWP','TEMP','PRES','Iws']].dropna(subset=['pm2.5'])\n",
        "# Set the index and specify the frequency as hourly ('H')\n",
        "df = df.set_index('datetime').sort_index().asfreq('H')\n",
        "y = df['pm2.5']\n",
        "exog = df[['DEWP','TEMP','PRES','Iws']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target length: 43800 Range: 2010-01-02 00:00:00 to 2014-12-31 23:00:00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "datetime\n",
              "2010-01-02 00:00:00    129.0\n",
              "2010-01-02 01:00:00    148.0\n",
              "2010-01-02 02:00:00    159.0\n",
              "2010-01-02 03:00:00    181.0\n",
              "2010-01-02 04:00:00    138.0\n",
              "Freq: H, Name: pm2.5, dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DEWP</th>\n",
              "      <th>TEMP</th>\n",
              "      <th>PRES</th>\n",
              "      <th>Iws</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>datetime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-02 00:00:00</th>\n",
              "      <td>-16.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>1020.0</td>\n",
              "      <td>1.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-02 01:00:00</th>\n",
              "      <td>-15.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>1020.0</td>\n",
              "      <td>2.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-02 02:00:00</th>\n",
              "      <td>-11.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>1021.0</td>\n",
              "      <td>3.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-02 03:00:00</th>\n",
              "      <td>-7.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>1022.0</td>\n",
              "      <td>5.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-02 04:00:00</th>\n",
              "      <td>-7.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>1022.0</td>\n",
              "      <td>6.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     DEWP  TEMP    PRES   Iws\n",
              "datetime                                     \n",
              "2010-01-02 00:00:00 -16.0  -4.0  1020.0  1.79\n",
              "2010-01-02 01:00:00 -15.0  -4.0  1020.0  2.68\n",
              "2010-01-02 02:00:00 -11.0  -5.0  1021.0  3.57\n",
              "2010-01-02 03:00:00  -7.0  -5.0  1022.0  5.36\n",
              "2010-01-02 04:00:00  -7.0  -5.0  1022.0  6.25"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Target length:\", len(y), \"Range:\", y.index.min(), \"to\", y.index.max())\n",
        "display(y.head())\n",
        "display(exog.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "Available models: Index(['Name', 'Reference', 'Turbo'], dtype='object')\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "Available models: Index(['Name', 'Reference', 'Turbo'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>20:31:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Fitting 3 Folds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Auto ARIMA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               \n",
              "                                                               \n",
              "Initiated  . . . . . . . . . . . . . . . . . .         20:31:58\n",
              "Status     . . . . . . . . . . . . . . . . . .  Fitting 3 Folds\n",
              "Estimator  . . . . . . . . . . . . . . . . . .       Auto ARIMA"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a8cee th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_a8cee_row0_col0, #T_a8cee_row0_col1, #T_a8cee_row0_col2, #T_a8cee_row0_col3, #T_a8cee_row0_col4, #T_a8cee_row0_col5, #T_a8cee_row0_col6, #T_a8cee_row0_col7, #T_a8cee_row0_col8 {\n",
              "  text-align: left;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a8cee\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a8cee_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_a8cee_level0_col1\" class=\"col_heading level0 col1\" >MASE</th>\n",
              "      <th id=\"T_a8cee_level0_col2\" class=\"col_heading level0 col2\" >RMSSE</th>\n",
              "      <th id=\"T_a8cee_level0_col3\" class=\"col_heading level0 col3\" >MAE</th>\n",
              "      <th id=\"T_a8cee_level0_col4\" class=\"col_heading level0 col4\" >RMSE</th>\n",
              "      <th id=\"T_a8cee_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
              "      <th id=\"T_a8cee_level0_col6\" class=\"col_heading level0 col6\" >SMAPE</th>\n",
              "      <th id=\"T_a8cee_level0_col7\" class=\"col_heading level0 col7\" >R2</th>\n",
              "      <th id=\"T_a8cee_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a8cee_level0_row0\" class=\"row_heading level0 row0\" >arima</th>\n",
              "      <td id=\"T_a8cee_row0_col0\" class=\"data row0 col0\" >ARIMA</td>\n",
              "      <td id=\"T_a8cee_row0_col1\" class=\"data row0 col1\" >1.2242</td>\n",
              "      <td id=\"T_a8cee_row0_col2\" class=\"data row0 col2\" >1.0480</td>\n",
              "      <td id=\"T_a8cee_row0_col3\" class=\"data row0 col3\" >82.3911</td>\n",
              "      <td id=\"T_a8cee_row0_col4\" class=\"data row0 col4\" >103.9730</td>\n",
              "      <td id=\"T_a8cee_row0_col5\" class=\"data row0 col5\" >1.6369</td>\n",
              "      <td id=\"T_a8cee_row0_col6\" class=\"data row0 col6\" >0.7336</td>\n",
              "      <td id=\"T_a8cee_row0_col7\" class=\"data row0 col7\" >-0.9897</td>\n",
              "      <td id=\"T_a8cee_row0_col8\" class=\"data row0 col8\" >74.2867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x72ff36f32590>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e875c35307484de68130c0100c175d43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/81 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAvailable models:\u001b[39m\u001b[33m\"\u001b[39m, exp.models().keys())\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Compare all available models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m best = \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m final = exp.finalize_model(best)\n\u001b[32m     27\u001b[39m preds = exp.predict_model(final)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pycaret/time_series/forecasting/oop.py:2315\u001b[39m, in \u001b[36mTSForecastingExperiment.compare_models\u001b[39m\u001b[34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, experiment_custom_tags, engine, verbose, parallel)\u001b[39m\n\u001b[32m   2312\u001b[39m         \u001b[38;5;28mself\u001b[39m._set_engine(estimator=estimator, engine=eng, severity=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2315\u001b[39m     return_values = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2316\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2319\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2321\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_select\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbudget_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbudget_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mturbo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mturbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2325\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaller_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2334\u001b[39m         \u001b[38;5;66;03m# Reset the models back to the default engines\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:794\u001b[39m, in \u001b[36m_SupervisedExperiment.compare_models\u001b[39m\u001b[34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, verbose, parallel, caller_params)\u001b[39m\n\u001b[32m    791\u001b[39m results_columns_to_ignore = [\u001b[33m\"\u001b[39m\u001b[33mObject\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mruntime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcutoff\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     model, model_fit_time = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcreate_model_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     model_results = \u001b[38;5;28mself\u001b[39m.pull(pop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    797\u001b[39m         np.sum(\n\u001b[32m    798\u001b[39m             model_results.drop(\n\u001b[32m   (...)\u001b[39m\u001b[32m    802\u001b[39m         != \u001b[32m0.0\u001b[39m\n\u001b[32m    803\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1533\u001b[39m, in \u001b[36m_SupervisedExperiment._create_model\u001b[39m\u001b[34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m model, model_fit_time\n\u001b[32m   1531\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m model, model_fit_time, model_results, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_model_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;66;03m# end runtime\u001b[39;00m\n\u001b[32m   1550\u001b[39m runtime_end = time.time()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pycaret/time_series/forecasting/oop.py:2644\u001b[39m, in \u001b[36mTSForecastingExperiment._create_model_with_cv\u001b[39m\u001b[34m(self, model, data_X, data_y, fit_kwargs, round, cv, metrics, refit, display, **kwargs)\u001b[39m\n\u001b[32m   2639\u001b[39m pipeline_with_model = _add_model_to_pipeline(\n\u001b[32m   2640\u001b[39m     pipeline=\u001b[38;5;28mself\u001b[39m.pipeline, model=model\n\u001b[32m   2641\u001b[39m )\n\u001b[32m   2643\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m redirect_output(\u001b[38;5;28mself\u001b[39m.logger):\n\u001b[32m-> \u001b[39m\u001b[32m2644\u001b[39m     scores, cutoffs = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_with_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpoint_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoverage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_scorer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2660\u001b[39m model_fit_end = time.time()\n\u001b[32m   2661\u001b[39m model_fit_time = np.array(model_fit_end - model_fit_start).round(\u001b[32m2\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pycaret/utils/time_series/forecasting/model_selection.py:306\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(pipeline, y, X, cv, scoring, fit_params, n_jobs, return_train_score, alpha, coverage, error_score, verbose, **additional_scorer_kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m     scoring = _get_metrics_dict(scoring)\n\u001b[32m    304\u001b[39m     parallel = Parallel(n_jobs=n_jobs)\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m            \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_scorer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# raise key exceptions\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1861\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1863\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1866\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1867\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1868\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1869\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1790\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pycaret/utils/time_series/forecasting/model_selection.py:157\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(pipeline, y, X, scoring, train, test, parameters, fit_params, return_train_score, alpha, coverage, error_score, **additional_scorer_kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m start = time.time()\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    159\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFit failed on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/sktime/forecasting/base/_base.py:380\u001b[39m, in \u001b[36mBaseForecaster.fit\u001b[39m\u001b[34m(self, y, X, fh)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# we call the ordinary _fit if no looping/vectorization needed\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectorization_needed:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# otherwise we call the vectorized version of fit\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m._vectorize(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, y=y_inner, X=X_inner, fh=fh)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/sktime/forecasting/compose/_pipeline.py:500\u001b[39m, in \u001b[36mForecastingPipeline._fit\u001b[39m\u001b[34m(self, y, X, fh)\u001b[39m\n\u001b[32m    498\u001b[39m name, forecaster = \u001b[38;5;28mself\u001b[39m.steps_[-\u001b[32m1\u001b[39m]\n\u001b[32m    499\u001b[39m f = forecaster.clone()\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28mself\u001b[39m.steps_[-\u001b[32m1\u001b[39m] = (name, f)\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/sktime/forecasting/base/_base.py:380\u001b[39m, in \u001b[36mBaseForecaster.fit\u001b[39m\u001b[34m(self, y, X, fh)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# we call the ordinary _fit if no looping/vectorization needed\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectorization_needed:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# otherwise we call the vectorized version of fit\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m._vectorize(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, y=y_inner, X=X_inner, fh=fh)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/sktime/forecasting/compose/_pipeline.py:1009\u001b[39m, in \u001b[36mTransformedTargetForecaster._fit\u001b[39m\u001b[34m(self, y, X, fh)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;66;03m# fit forecaster\u001b[39;00m\n\u001b[32m   1008\u001b[39m f = \u001b[38;5;28mself\u001b[39m.forecaster_\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# transform post\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformers_post_:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/sktime/forecasting/base/_base.py:380\u001b[39m, in \u001b[36mBaseForecaster.fit\u001b[39m\u001b[34m(self, y, X, fh)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# we call the ordinary _fit if no looping/vectorization needed\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectorization_needed:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# otherwise we call the vectorized version of fit\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m._vectorize(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, y=y_inner, X=X_inner, fh=fh)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/sktime/forecasting/base/adapters/_pmdarima.py:55\u001b[39m, in \u001b[36m_PmdArimaAdapter._fit\u001b[39m\u001b[34m(self, y, X, fh)\u001b[39m\n\u001b[32m     53\u001b[39m     X = X.loc[y.index]\n\u001b[32m     54\u001b[39m \u001b[38;5;28mself\u001b[39m._forecaster = \u001b[38;5;28mself\u001b[39m._instantiate_model()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forecaster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/auto.py:167\u001b[39m, in \u001b[36mAutoARIMA.fit\u001b[39m\u001b[34m(self, y, X, **fit_args)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit the auto-arima estimator\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03mFit an AutoARIMA to a vector, ``y``, of observations with an\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m \u001b[33;03m    Any keyword arguments to pass to the auto-arima function.\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    165\u001b[39m sarimax_kwargs = {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.kwargs\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28mself\u001b[39m.model_ = \u001b[43mauto_arima\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_q\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_q\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_P\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mD\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_Q\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_Q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_P\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_D\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_Q\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_Q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_order\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseasonal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseasonal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstationary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstationary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43minformation_criterion\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minformation_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseasonal_test\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseasonal_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstepwise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstepwise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset_test_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moffset_test_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseasonal_test_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseasonal_test_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_action\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_fits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_fits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_valid_fits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# only return ONE\u001b[39;49;00m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_of_sample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_of_sample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwith_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43msarimax_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msarimax_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/auto.py:701\u001b[39m, in \u001b[36mauto_arima\u001b[39m\u001b[34m(y, X, start_p, d, start_q, max_p, max_d, max_q, start_P, D, start_Q, max_P, max_D, max_Q, max_order, m, seasonal, stationary, information_criterion, alpha, test, seasonal_test, stepwise, n_jobs, start_params, trend, method, maxiter, offset_test_args, seasonal_test_args, suppress_warnings, error_action, trace, random, random_state, n_fits, return_valid_fits, out_of_sample_size, scoring, scoring_args, with_intercept, sarimax_kwargs, **fit_args)\u001b[39m\n\u001b[32m    669\u001b[39m     \u001b[38;5;66;03m# init the stepwise model wrapper\u001b[39;00m\n\u001b[32m    670\u001b[39m     search = solvers._StepwiseFitWrapper(\n\u001b[32m    671\u001b[39m         y,\n\u001b[32m    672\u001b[39m         X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    698\u001b[39m         **sarimax_kwargs,\n\u001b[32m    699\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m sorted_res = \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _return_wrapper(sorted_res, return_valid_fits, start, trace)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/_auto_solvers.py:288\u001b[39m, in \u001b[36m_StepwiseFitWrapper.solve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPerforming stepwise search to minimize \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m           % \u001b[38;5;28mself\u001b[39m.information_criterion)\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# fit a baseline p, d, q model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# null model with possible constant\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._do_fit((\u001b[32m0\u001b[39m, d, \u001b[32m0\u001b[39m), (\u001b[32m0\u001b[39m, D, \u001b[32m0\u001b[39m, m)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/_auto_solvers.py:235\u001b[39m, in \u001b[36m_StepwiseFitWrapper._do_fit\u001b[39m\u001b[34m(self, order, seasonal_order, constant)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (order, seasonal_order, constant) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results_dict:\n\u001b[32m    231\u001b[39m \n\u001b[32m    232\u001b[39m     \u001b[38;5;66;03m# increment the number of fits\u001b[39;00m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.k += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     fit, fit_time, new_ic = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_arima\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# use the orders as a key to be hashed for\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# the dictionary (pointing to fit)\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m.results_dict[(order, seasonal_order, constant)] = fit\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/_auto_solvers.py:508\u001b[39m, in \u001b[36m_fit_candidate_model\u001b[39m\u001b[34m(y, X, order, seasonal_order, start_params, trend, method, maxiter, fit_params, suppress_warnings, trace, error_action, out_of_sample_size, scoring, scoring_args, with_intercept, information_criterion, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m fit = ARIMA(order=order, seasonal_order=seasonal_order,\n\u001b[32m    501\u001b[39m             start_params=start_params, trend=trend, method=method,\n\u001b[32m    502\u001b[39m             maxiter=maxiter, suppress_warnings=suppress_warnings,\n\u001b[32m    503\u001b[39m             out_of_sample_size=out_of_sample_size, scoring=scoring,\n\u001b[32m    504\u001b[39m             scoring_args=scoring_args,\n\u001b[32m    505\u001b[39m             with_intercept=with_intercept, **kwargs)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[43mfit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# for non-stationarity errors or singular matrices, return None\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (LinAlgError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m v:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/arima.py:603\u001b[39m, in \u001b[36mARIMA.fit\u001b[39m\u001b[34m(self, y, X, **fit_args)\u001b[39m\n\u001b[32m    600\u001b[39m         X = safe_indexing(X, \u001b[38;5;28mslice\u001b[39m(\u001b[32m0\u001b[39m, n_exog - cv))\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# Internal call\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# now make a forecast if we're validating to compute the\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# out-of-sample score\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m     \u001b[38;5;66;03m# get the predictions (use self.predict, which calls forecast\u001b[39;00m\n\u001b[32m    609\u001b[39m     \u001b[38;5;66;03m# from statsmodels internally)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/arima.py:524\u001b[39m, in \u001b[36mARIMA._fit\u001b[39m\u001b[34m(self, y, X, **fit_args)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings(record=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    523\u001b[39m         warnings.simplefilter(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m         fit, \u001b[38;5;28mself\u001b[39m.arima_res_ = \u001b[43m_fit_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    526\u001b[39m     fit, \u001b[38;5;28mself\u001b[39m.arima_res_ = _fit_wrapper()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/pmdarima/arima/arima.py:510\u001b[39m, in \u001b[36mARIMA._fit.<locals>._fit_wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    507\u001b[39m _maxiter = fit_args.pop(\u001b[33m\"\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m\"\u001b[39m, _maxiter)\n\u001b[32m    509\u001b[39m disp = fit_args.pop(\u001b[33m\"\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m fitted = \u001b[43marima\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_maxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arima, fitted\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/tsa/statespace/mlemodel.py:705\u001b[39m, in \u001b[36mMLEModel.fit\u001b[39m\u001b[34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m         flags[\u001b[33m'\u001b[39m\u001b[33mhessian_method\u001b[39m\u001b[33m'\u001b[39m] = optim_hessian\n\u001b[32m    704\u001b[39m     fargs = (flags,)\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     mlefit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/base/model.py:566\u001b[39m, in \u001b[36mLikelihoodModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_t\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    565\u001b[39m optimizer = Optimizer()\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m xopt, retvals, optim_settings = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[32m    576\u001b[39m optim_settings.update(kwds)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/base/optimizer.py:245\u001b[39m, in \u001b[36mOptimizer._fit\u001b[39m\u001b[34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[39m\n\u001b[32m    242\u001b[39m     fit_funcs.update(extra_fit_funcs)\n\u001b[32m    244\u001b[39m func = fit_funcs[method]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m xopt, retvals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m optim_settings = {\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: method, \u001b[33m'\u001b[39m\u001b[33mstart_params\u001b[39m\u001b[33m'\u001b[39m: start_params,\n\u001b[32m    251\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m'\u001b[39m: maxiter, \u001b[33m'\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m'\u001b[39m: full_output,\n\u001b[32m    252\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp, \u001b[33m'\u001b[39m\u001b[33mfargs\u001b[39m\u001b[33m'\u001b[39m: fargs, \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    253\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mretall\u001b[39m\u001b[33m'\u001b[39m: retall, \u001b[33m\"\u001b[39m\u001b[33mextra_fit_funcs\u001b[39m\u001b[33m\"\u001b[39m: extra_fit_funcs}\n\u001b[32m    254\u001b[39m optim_settings.update(kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/base/optimizer.py:665\u001b[39m, in \u001b[36m_fit_lbfgs\u001b[39m\u001b[34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SP_LT_116:\n\u001b[32m    664\u001b[39m     extended_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m\"\u001b[39m]=disp\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m retvals = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextended_kwargs\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[32m    677\u001b[39m     xopt, fopt, d = retvals\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:199\u001b[39m, in \u001b[36mfmin_l_bfgs_b\u001b[39m\u001b[34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[39m\n\u001b[32m    187\u001b[39m callback = _wrap_callback(callback)\n\u001b[32m    188\u001b[39m opts = {\u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp,\n\u001b[32m    189\u001b[39m         \u001b[33m'\u001b[39m\u001b[33miprint\u001b[39m\u001b[33m'\u001b[39m: iprint,\n\u001b[32m    190\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmaxcor\u001b[39m\u001b[33m'\u001b[39m: m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    196\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    197\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmaxls\u001b[39m\u001b[33m'\u001b[39m: maxls}\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m                       \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m d = {\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mjac\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    202\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    203\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mfuncalls\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mnfev\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    204\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mnit\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mnit\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    205\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mwarnflag\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m]}\n\u001b[32m    206\u001b[39m f = res[\u001b[33m'\u001b[39m\u001b[33mfun\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:365\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[39m\n\u001b[32m    359\u001b[39m task_str = task.tobytes()\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task_str.startswith(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFG\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    361\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    363\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    364\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task_str.startswith(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNEW_X\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    367\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    368\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:286\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x_impl(x)\n\u001b[32m    285\u001b[39m \u001b[38;5;28mself\u001b[39m._update_fun()\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:256\u001b[39m, in \u001b[36mScalarFunction._update_grad\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.g_updated:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.g_updated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:173\u001b[39m, in \u001b[36mScalarFunction.__init__.<locals>.update_grad\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m._update_fun()\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.ngev += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28mself\u001b[39m.g = \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                           \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:505\u001b[39m, in \u001b[36mapprox_derivative\u001b[39m\u001b[34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     use_one_sided = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) == \u001b[32m2\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:576\u001b[39m, in \u001b[36m_dense_difference\u001b[39m\u001b[34m(fun, x0, f0, h, use_one_sided, method)\u001b[39m\n\u001b[32m    574\u001b[39m     x = x0 + h_vecs[i]\n\u001b[32m    575\u001b[39m     dx = x[i] - x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     df = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m - f0\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33m3-point\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[32m    578\u001b[39m     x1 = x0 + h_vecs[i]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:456\u001b[39m, in \u001b[36mapprox_derivative.<locals>.fun_wrapped\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfun_wrapped\u001b[39m(x):\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     f = np.atleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    458\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`fun` return value has \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    459\u001b[39m                            \u001b[33m\"\u001b[39m\u001b[33mmore than 1 dimension.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[39m, in \u001b[36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m fx = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isscalar(fx):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/base/model.py:534\u001b[39m, in \u001b[36mLikelihoodModel.fit.<locals>.f\u001b[39m\u001b[34m(params, *args)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(params, *args):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m / nobs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/tsa/statespace/mlemodel.py:940\u001b[39m, in \u001b[36mMLEModel.loglike\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[32m    938\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minversion_method\u001b[39m\u001b[33m'\u001b[39m] = INVERT_UNIVARIATE | SOLVE_LU\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m loglike = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/tsa/statespace/kalman_filter.py:1001\u001b[39m, in \u001b[36mKalmanFilter.loglike\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[32m    987\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    997\u001b[39m \u001b[33;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    999\u001b[39m kwargs.setdefault(\u001b[33m'\u001b[39m\u001b[33mconserve_memory\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1000\u001b[39m                   MEMORY_CONSERVE ^ MEMORY_NO_LIKELIHOOD)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m kfilter = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m loglikelihood_burn = kwargs.get(\u001b[33m'\u001b[39m\u001b[33mloglikelihood_burn\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1003\u001b[39m                                 \u001b[38;5;28mself\u001b[39m.loglikelihood_burn)\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[33m'\u001b[39m\u001b[33mconserve_memory\u001b[39m\u001b[33m'\u001b[39m] & MEMORY_NO_LIKELIHOOD):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/akshata/.venv/lib/python3.11/site-packages/statsmodels/tsa/statespace/kalman_filter.py:924\u001b[39m, in \u001b[36mKalmanFilter._filter\u001b[39m\u001b[34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_state(prefix=prefix, complex_step=complex_step)\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m \u001b[43mkfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m kfilter\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from pycaret.time_series import TSForecastingExperiment\n",
        "\n",
        "# Combine target and exogenous variables into one dataframe\n",
        "data = pd.concat([y, exog], axis=1)\n",
        "\n",
        "exp = TSForecastingExperiment()\n",
        "exp.setup(\n",
        "    data=data,\n",
        "    target='pm2.5',  # Specify which column is the target\n",
        "    fh=24, \n",
        "    fold=3, \n",
        "    session_id=42, \n",
        "    seasonal_period=24, \n",
        "    n_jobs=1,\n",
        "    verbose=False, \n",
        "    use_gpu=True,\n",
        "    numeric_imputation_target='mean',\n",
        "    numeric_imputation_exogenous='mean'\n",
        ")\n",
        "\n",
        "# Get list of available models\n",
        "print(\"Available models:\", exp.models().keys())\n",
        "\n",
        "# Compare all available models\n",
        "best = exp.compare_models(verbose=True)\n",
        "final = exp.finalize_model(best)\n",
        "preds = exp.predict_model(final)\n",
        "fig = exp.plot_model(final, plot='forecast')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c924aaca",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "akshata",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
